{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:78px\">Final Project IRWA (2024-2025)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:48px\">Part 2: Indexing and Evaluation</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Third-party imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Local application imports\n",
    "current_dir = os.path.dirname(os.path.abspath(__file__)) if '__file__' in locals() else os.getcwd()\n",
    "project_root = os.path.join(current_dir, '..')\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "import irwa.loading as ild \n",
    "import irwa.preprocessing as ipp\n",
    "import irwa.indexing as ind\n",
    "import irwa.ranking as irk\n",
    "\n",
    "# The following lines allow for autoreload of modules. They allow changes in modules without the need to reload the kernel.\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 117407 tweets\n",
      "Loaded 48429 documents with their corresponding tokenized tweet content\n"
     ]
    }
   ],
   "source": [
    "# Loading\n",
    "file_path = '../data/farmers-protest-tweets.json'\n",
    "tweets = ild.load_tweets_from_json(file_path)\n",
    "print(f\"Loaded {len(tweets)} tweets\")\n",
    "\n",
    "# Preprocessing\n",
    "tweet_document_ids_map_df = \"../data/tweet_document_ids_map.csv\"\n",
    "docid_to_tweetid, token_tweets = ipp.create_tokenized_dictionary(tweets, tweet_document_ids_map_df)\n",
    "print(f\"Loaded {len(token_tweets)} documents with their corresponding tokenized tweet content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index, tf, idf = ind.create_inverted_index_tf_idf(token_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of test queries\n",
    "query1 = \"Indian protest\"      # Example given in handout\n",
    "query2 = \"support farmers\"     # Example given in handout\n",
    "query3 = \"Delhi farmers\"\n",
    "query4 = \"Government corrupt\"\n",
    "query5 = \"president India\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Results:\n",
      "------------------------------------------------------------\n",
      "RESULT 1\n",
      "Document doc_9676: 1.418501666\n",
      "Content: This is why Indian Farmers are protesting #FarmersProtest https://t.co/9mzFBGQaXL\n",
      "------------------------------------------------------------\n",
      "RESULT 2\n",
      "Document doc_34729: 1.406224306875\n",
      "Content: Indian farmers' protests: Why they matter to British Indians\n",
      "#FarmersProtest  https://t.co/kyCWnDVyEm\n",
      "------------------------------------------------------------\n",
      "RESULT 3\n",
      "Document doc_39111: 1.1820847216666666\n",
      "Content: Indian farmers have right to peacefull protest #FarmersProtest\n",
      "------------------------------------------------------------\n",
      "RESULT 4\n",
      "Document doc_30422: 1.1820847216666666\n",
      "Content: Why are Indian farmers protesting against the government?\n",
      "#FarmersProtest  https://t.co/eMUGoXtabZ\n",
      "------------------------------------------------------------\n",
      "RESULT 5\n",
      "Document doc_33904: 1.1249794455000002\n",
      "Content: Indian farmers' protests: Why they matter to British Indians\n",
      "\n",
      "#FarmersStandingFirm #FarmersProtest #StandWithFarmers \n",
      "\n",
      "https://t.co/ywgPhLCvm9\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Ranking results with TF-IDF\n",
    "ranked_documents_1 = irk.rank_documents(ipp.build_terms(query1), token_tweets, inverted_index, tf, idf, filter=irk.conjunctive_filtering)\n",
    "irk.display_scores_tf_idf(ranked_documents_1, docid_to_tweetid, tweets, 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Results:\n",
      "------------------------------------------------------------\n",
      "RESULT 1\n",
      "Document doc_31878: 1.131033092\n",
      "Content: Support farmers, support humanity #Farmersprotest\n",
      "------------------------------------------------------------\n",
      "RESULT 2\n",
      "Document doc_38864: 1.131033092\n",
      "Content: support farmers support #FarmersProtest \n",
      "#‡§∂‡§π‡•Ä‡§¶_‡§ú‡§µ‡§æ‡§®_‡§∂‡§π‡•Ä‡§¶_‡§ï‡§ø‡§∏‡§æ\n",
      "------------------------------------------------------------\n",
      "RESULT 3\n",
      "Document doc_45741: 1.0921162466666667\n",
      "Content: Support Farmers üôèüôèüôèüôèüôèüôè#FarmersProtest\n",
      "------------------------------------------------------------\n",
      "RESULT 4\n",
      "Document doc_2815: 1.0921162466666667\n",
      "Content: Support Farmers üôèüôèüôèüôèüôèüôè#FarmersProtest\n",
      "------------------------------------------------------------\n",
      "RESULT 5\n",
      "Document doc_30390: 1.0921162466666667\n",
      "Content: Support farmers #FarmersProtest\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Ranking results with TF-IDF\n",
    "ranked_documents_2 = irk.rank_documents(ipp.build_terms(query2), token_tweets, inverted_index, tf, idf, filter=irk.conjunctive_filtering)\n",
    "irk.display_scores_tf_idf(ranked_documents_2, docid_to_tweetid, tweets, 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Results:\n",
      "------------------------------------------------------------\n",
      "RESULT 1\n",
      "Document doc_14534: 1.1625731736363636\n",
      "Content: Farmers are in Delhi for their rights, Delhi Police consider them as your fellow countrymen. #DPstopIntimidatingFarmers \n",
      "#FarmersProtest https://t.co/9VqzgkG0Sr\n",
      "------------------------------------------------------------\n",
      "RESULT 2\n",
      "Document doc_29107: 0.9837157623076924\n",
      "Content: Sadly he could not see the lakhs of farmers protesting outside Delhi when he was flying out of Delhi .\n",
      "\n",
      "#IamAgainstModiGovt \n",
      "#FarmersProtest https://t.co/JHrtbfYjOg\n",
      "------------------------------------------------------------\n",
      "RESULT 3\n",
      "Document doc_38281: 0.9837157623076924\n",
      "Content: I blame the Delhi fog. Otherwise he would have seen protesting farmers when he flew out of Delhi this morning. #FarmersProtest https://t.co/DFMS52Ne0R\n",
      "------------------------------------------------------------\n",
      "RESULT 4\n",
      "Document doc_30534: 0.9837157623076924\n",
      "Content: Sadly he could not see the lakhs of farmers protesting outside Delhi when he was flying out of Delhi .\n",
      "\n",
      "#IamAgainstModiGovt \n",
      "#FarmersProtest https://t.co/UDuCBv5aMg\n",
      "------------------------------------------------------------\n",
      "RESULT 5\n",
      "Document doc_19201: 0.9775597807142858\n",
      "Content: The voice of farmers, who are  protesting in delhi must be heard #FarmersProtest\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Ranking results with TF-IDF\n",
    "ranked_documents_3 = irk.rank_documents(ipp.build_terms(query3), token_tweets, inverted_index, tf, idf, filter=irk.conjunctive_filtering)\n",
    "irk.display_scores_tf_idf(ranked_documents_3, docid_to_tweetid, tweets, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Results:\n",
      "------------------------------------------------------------\n",
      "RESULT 1\n",
      "Document doc_14701: 3.4573308637499998\n",
      "Content: Corruption Corruption thats what this Government will be Remembered for #DPstopIntimidatingFarmers\n",
      "#FarmersProtest https://t.co/7QsxGwbWmr\n",
      "------------------------------------------------------------\n",
      "RESULT 2\n",
      "Document doc_14680: 3.0731829899999994\n",
      "Content: Corruption Corruption thats what this Government will be Remembered for #DPstopIntimidatingFarmers\n",
      "#FarmersProtest https://t.co/Oxe9SDB01w\n",
      "#DPstopIntimidatingFarmers\n",
      "------------------------------------------------------------\n",
      "RESULT 3\n",
      "Document doc_14671: 3.0731829899999994\n",
      "Content: #MSP_‡§ï‡§ø‡§∏‡§æ‡§®_‡§ï‡§æ_‡§π‡§ï \n",
      "Corruption Corruption thats what this Government will be Remembered for #DPstopIntimidatingFarmers\n",
      "#FarmersProtest https://t.co/aTNdwitLS9\n",
      "------------------------------------------------------------\n",
      "RESULT 4\n",
      "Document doc_37261: 1.7815834961111108\n",
      "Content: @rihanna Shame on India fake media and corrupt government #FarmersProtest #BidenAdministration\n",
      "------------------------------------------------------------\n",
      "RESULT 5\n",
      "Document doc_1441: 1.6034251465\n",
      "Content: You just need to comprehend what this corrupt lying bjp government of ours is doing\n",
      "\n",
      "#FarmersProtest \n",
      "#Pagdi_Sambhal_Jatta https://t.co/OO1EJ17tua https://t.co/H79R36qTAo\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Ranking results with TF-IDF\n",
    "ranked_documents_4 = irk.rank_documents(ipp.build_terms(query4), token_tweets, inverted_index, tf, idf, filter=irk.conjunctive_filtering)\n",
    "irk.display_scores_tf_idf(ranked_documents_4, docid_to_tweetid, tweets, 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Results:\n",
      "------------------------------------------------------------\n",
      "RESULT 1\n",
      "Document doc_31745: 2.4150228049999996\n",
      "Content: it's president's day. \n",
      "\n",
      "@joebiden any thoughts about what's been happening in india?? #farmersprotest #nofarmersnofood\n",
      "------------------------------------------------------------\n",
      "RESULT 2\n",
      "Document doc_22798: 1.8112671037499997\n",
      "Content: @POTUS @JoeBiden Mr. President please support Indian farmers #FarmersProtest democracy killed in India..\n",
      "------------------------------------------------------------\n",
      "RESULT 3\n",
      "Document doc_30908: 1.7125492237499995\n",
      "Content: US Lawyers write to President #Biden on #Farmers' Protests:\n",
      "'Your administration comes to office at a time when minority communities across India are in peril,' notes the letter, urging the US President to act.\n",
      "#FarmersProtest https://t.co/wlQVau3nkw\n",
      "------------------------------------------------------------\n",
      "RESULT 4\n",
      "Document doc_27740: 1.1439581707894735\n",
      "Content: I request US President @JoeBiden to protect @dhruv_rathee from Dangerous Supreme leaders of India; @AmitShah and @PMOIndia. Because he is raising his voice for human rights and democracy. #FarmersProtest #AskDhruvRathee\n",
      "------------------------------------------------------------\n",
      "RESULT 5\n",
      "Document doc_21433: 1.1439581707894735\n",
      "Content: @KanganaTeam Who are you to comment on him or say anything to Americans ???? I did not know that America or their President was part of India ????? Weren't you deshbhakats giving the gyan to foreigners not to speak on Indian matters such as #FarmersProtest\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Ranking results with TF-IDF\n",
    "ranked_documents_5 = irk.rank_documents(ipp.build_terms(query5), token_tweets, inverted_index, tf, idf, filter=irk.conjunctive_filtering)\n",
    "irk.display_scores_tf_idf(ranked_documents_5, docid_to_tweetid, tweets, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_to_ev_1 = \"People's rights\"\n",
    "query_to_ev_2 = \"Indian Government\"\n",
    "\n",
    "queryid2text = {\n",
    "    1: query_to_ev_1,\n",
    "    2: query_to_ev_2\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Given queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_evq1 = irk.tf_idf(inverted_index, query_to_ev_1, token_tweets)\n",
    "scores_evq2 = irk.tf_idf(inverted_index, query_to_ev_2, token_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dictionaries to DataFrames\n",
    "df_query_1 = pd.DataFrame(list(scores_evq1.items()), columns=['doc_id', 'doc_score'])\n",
    "df_query_2 = pd.DataFrame(list(scores_evq2.items()), columns=['doc_id', 'doc_score'])\n",
    "\n",
    "# Add a column for query_id\n",
    "df_query_1['query_id'] = 1\n",
    "df_query_2['query_id'] = 2\n",
    "\n",
    "# Concatenate the two DataFrames\n",
    "search_results = pd.concat([df_query_1, df_query_2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = pd.read_csv(\"../data/evaluation_gt.csv\", delimiter=\";\")\n",
    "results = pd.merge(evaluation, search_results, how='left', left_on=['docId', 'query_id'], right_on=['doc_id', 'query_id'])\n",
    "results.drop(columns=['doc_id'], inplace=True)\n",
    "# Fill Nan with 0 as it means that is has not found any relevant score for such query\n",
    "results.fillna(0, inplace=True)\n",
    "\n",
    "#Rename columns for better usage\n",
    "results.columns = [\"doc_id\", \"query_id\", \"is_relevant\", \"predicted_relevance\" ]\n",
    "mean_predicted_relevance = results['predicted_relevance'].mean()\n",
    "std_predicted_relevance = results['predicted_relevance'].std()\n",
    "\n",
    "# Standardizing the predicted_relevance column\n",
    "results['predicted_relevance'] = (results['predicted_relevance'] - mean_predicted_relevance) / std_predicted_relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "-------------------------------------------------------------------------------------------\n",
      "Out of 30 documents, 15 are found relevant for query 'people's rights'\n",
      "-------------------------------------------------------------------------------------------\n",
      "2\n",
      "-------------------------------------------------------------------------------------------\n",
      "Out of 30 documents, 15 are found relevant for query 'Indian Government'\n",
      "-------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for query in results['query_id'].unique():\n",
    "    print(query)\n",
    "    # Count relevant documents for the current query\n",
    "    relevant_count = results[results['query_id'] == query]['is_relevant'].sum()\n",
    "    \n",
    "    print(\"-------------------------------------------------------------------------------------------\")\n",
    "    print(f\"Out of {len(results[results['query_id'] == query])} documents, {relevant_count} are found relevant for query '{queryid2text.get(query, 'Unknown query')}'\")\n",
    "    print(\"-------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision@K (P@K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(doc_score, y_score, k=10):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc_score: Ground truth (true relevance labels).\n",
    "    y_score: Predicted scores.\n",
    "    k : number of doc to consider.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    precision @k : float\n",
    "    recall @k : float\n",
    "\n",
    "    \"\"\"\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    doc_score = doc_score[order[:k]]\n",
    "    relevant = sum(doc_score == 1)\n",
    "    precision = float(relevant) / k\n",
    "\n",
    "    total_relevant = sum(doc_score)\n",
    "    if total_relevant == 0:\n",
    "        recall = 0\n",
    "    else:\n",
    "        recall = float(relevant) / total_relevant\n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuari\\AppData\\Local\\Temp\\ipykernel_5496\\2359606978.py:2: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  current_query_res = results[search_results[\"query_id\"] == 1]\n"
     ]
    }
   ],
   "source": [
    "# Assign the current query\n",
    "current_query_res = results[search_results[\"query_id\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Precision@25: 0.92\n",
      "\n",
      "==> Precision@20: 0.95\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pre_k25, rec_k25 = precision_at_k(current_query_res[\"is_relevant\"], current_query_res[\"predicted_relevance\"], 25)\n",
    "pre_k20, rec_k20 = precision_at_k(current_query_res[\"is_relevant\"], current_query_res[\"predicted_relevance\"], 20)\n",
    "\n",
    "\n",
    "print(\"==> Precision@{}: {}\\n\".format(25, pre_k25))\n",
    "print(\"==> Precision@{}: {}\\n\".format(20, pre_k20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall@K (R@k) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Recall@25: 1.0\n",
      "\n",
      "==> Recall@20: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"==> Recall@{}: {}\\n\".format(25, rec_k25))\n",
    "print(\"==> Recall@{}: {}\\n\".format(20, rec_k20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avgerage Precision@K (P@K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_precision_at_k(doc_score, y_score, k=10):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc_score: Ground truth (true relevance labels).\n",
    "    y_score: Predicted scores.\n",
    "    k : number of doc to consider.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    average precision @k : floa\n",
    "    \"\"\"\n",
    "    order = np.argsort(y_score)[::-1]  # get the list of indexes of the predicted score sorted in descending order.\n",
    "\n",
    "    prec_at_i = 0\n",
    "    prec_at_i_list = []\n",
    "    number_of_relevant = 0\n",
    "    number_to_iterate = min(k, len(order))\n",
    "\n",
    "    for i in range(number_to_iterate):\n",
    "        if doc_score[order[i]] == 1:\n",
    "            number_of_relevant += 1\n",
    "            prec_at_i = number_of_relevant / (i + 1)\n",
    "            prec_at_i_list.append(prec_at_i)\n",
    "\n",
    "    if number_of_relevant == 0:\n",
    "        return 0\n",
    "    else:\n",
    "      return np.sum(prec_at_i_list) / number_of_relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9870474390134127)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_precision_at_k(np.array(current_query_res[\"is_relevant\"]), np.array(current_query_res[\"predicted_relevance\"]), 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9860869565217392)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "k = 25\n",
    "temp = current_query_res.sort_values(\"predicted_relevance\", ascending=False).head(k)\n",
    "average_precision_score(np.array(temp[\"is_relevant\"]), np.array(temp[\"predicted_relevance\"][:k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1-Score@K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(precision, recall):\n",
    "\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-1 Score for k = 20 is: 0.9743589743589743\n",
      "F-1 Score for k = 25 is: 0.9583333333333334\n"
     ]
    }
   ],
   "source": [
    "print(f\"F-1 Score for k = 20 is: {f1_score(pre_k20, rec_k20)}\")\n",
    "print(f\"F-1 Score for k = 25 is: {f1_score(pre_k25, rec_k25)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Average Precision (MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_at_k(search_res, k=10):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    search_res: search results dataset containing:\n",
    "        query_id: query id.\n",
    "        doc_id: document id.\n",
    "        predicted_relevance: relevance predicted through LightGBM.\n",
    "        doc_score: actual score of the document for the query (ground truth).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mean average precision @ k : float\n",
    "    \"\"\"\n",
    "    avp = []\n",
    "    for q in search_res[\"query_id\"].unique():  # loop over all query id\n",
    "        curr_data = search_res[search_res[\"query_id\"] == q]  # select data for current query\n",
    "        avp.append(avg_precision_at_k(np.array(curr_data[\"is_relevant\"]),\n",
    "                   np.array(curr_data[\"predicted_relevance\"]), k))  #append average precision for current query\n",
    "    return np.sum(avp) / len(avp), avp  # return mean average precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9625440630797774)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_k, avp = map_at_k(results, 25)\n",
    "map_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Reciprocal Rank (MRR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rr_at_k(doc_score, y_score, k=10):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc_score: Ground truth (true relevance labels).\n",
    "    y_score: Predicted scores.\n",
    "    k : number of doc to consider.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Reciprocal Rank for qurrent query\n",
    "    \"\"\"\n",
    "\n",
    "    order = np.argsort(y_score)[::-1]  # get the list of indexes of the predicted score sorted in descending order.\n",
    "    doc_score = np.take(doc_score, order[:k])  # sort the actual relevance label of the documents based on predicted score(hint: np.take) and take first k.\n",
    "    if np.sum(doc_score) == 0:  # if there are not relevant doument return 0\n",
    "        return 0\n",
    "    return 1 / (np.argmax(doc_score == 1) + 1)  # hint: to get the position of the first relevant document use \"np.argmax\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.0)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 5\n",
    "labels = np.array(results[results['query_id'] == 1][\"is_relevant\"])\n",
    "scores = np.array(results[results['query_id'] == 1][\"predicted_relevance\"])\n",
    "np.round(rr_at_k(labels, scores, 13), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalized Discounted Cumulative Gain (NDCG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_at_k(doc_score, y_score, k=10):\n",
    "    order = np.argsort(y_score)[::-1]  # get the list of indexes of the predicted score sorted in descending order.\n",
    "    doc_score = np.take(doc_score, order[:k])  # sort the actual relevance label of the documents based on predicted score(hint: np.take) and take first k.\n",
    "    gain = 2 ** doc_score - 1  # Compute gain (use formula 7 above)\n",
    "    discounts = np.log2(np.arange(len(doc_score)) + 2)  # Compute denominator\n",
    "    return np.sum(gain / discounts)  #return dcg@k\n",
    "\n",
    "\n",
    "def ndcg_at_k(doc_score, y_score, k=10):\n",
    "    dcg_max = dcg_at_k(doc_score, doc_score, k)\n",
    "    if not dcg_max:\n",
    "        return 0\n",
    "    return np.round(dcg_at_k(doc_score, y_score, k) / dcg_max, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ndcg@5 for query with query_id=1: 1.0\n"
     ]
    }
   ],
   "source": [
    "ndcg_k = np.round(ndcg_at_k(labels, scores, k), 4)\n",
    "print(\"ndcg@{} for query with query_id={}: {}\".format(k, 1, ndcg_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ndcg@5 for query with query_id=2: 1.0\n"
     ]
    }
   ],
   "source": [
    "labels = np.array(results[results['query_id'] == 2][\"is_relevant\"])\n",
    "scores = np.array(results[results['query_id'] == 2][\"predicted_relevance\"])\n",
    "ndcg_k = np.round(ndcg_at_k(labels, scores, k), 4)\n",
    "print(\"ndcg@{} for query with query_id={}: {}\".format(k, 2, ndcg_k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Own queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision@K (P@K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall@K (R@K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Precision@K (P@K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1-Score@K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Average Precision (MAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Reciprocal Rank (MRR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalized Discounted Cumulative Gain (NDCG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T-SNE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
